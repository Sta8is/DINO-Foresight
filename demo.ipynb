{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "sys.path.append(parent_dir)\n",
    "import cv2\n",
    "import io\n",
    "import imageio\n",
    "from src.data import CS_VideoData \n",
    "from src.dino_f import Dino_f\n",
    "import argparse\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import os\n",
    "import matplotlib.cm as cm\n",
    "import glob\n",
    "import os.path as osp\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.colors as colors\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "import einops\n",
    "import math\n",
    "import torch.nn as nn\n",
    "IGNORE_LABEL = 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_images(tensor):\n",
    "    \"\"\"\n",
    "    Denormalize image tensor from ImageNet normalization\n",
    "    Args:\n",
    "        tensor: shape [B,T,C,H,W]\n",
    "    Returns:\n",
    "        denormalized tensor with values in [0,1]\n",
    "    \"\"\"\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(1,1,3,1,1).to(tensor.device)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(1,1,3,1,1).to(tensor.device)\n",
    "    # Denormalize: x = (norm_x * std) + mean\n",
    "    denorm_images = tensor * std + mean\n",
    "    # Clip to valid image range [0,1]\n",
    "    denorm_images = torch.clamp(denorm_images, 0, 1)\n",
    "    return denorm_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace()\n",
    "# Data Parameters\n",
    "args.data_path = '/home/ubuntu/cityscapes/leftImg8bit_sequence'\n",
    "args.dst_path = None\n",
    "args.img_size = (448,896)\n",
    "args.num_workers = 8\n",
    "args.num_workers_val = None\n",
    "args.sequence_length = 5\n",
    "args.batch_size = 1\n",
    "args.random_crop = True\n",
    "args.random_horizontal_flip = True\n",
    "args.random_time_flip = False\n",
    "args.timestep_augm = None\n",
    "args.no_timestep_augm = False\n",
    "args.use_fc_bias = True\n",
    "args.feature_extractor = 'dino'\n",
    "args.dinov2_variant = 'vitb14_reg'\n",
    "args.d_layers = [2,5,8,11]\n",
    "args.hidden_dim = 1152\n",
    "args.heads = 8\n",
    "args.layers = 12\n",
    "args.dropout = 0.1\n",
    "args.loss_type = 'SmoothL1'\n",
    "args.beta_smoothl1 = 0.1\n",
    "args.attn_dropout = 0.3\n",
    "args.step = 1\n",
    "args.masking = 'simple_replace'\n",
    "args.train_mask_mode = 'fullmask'\n",
    "args.seperable_attention = True\n",
    "args.seperable_window_size = 1\n",
    "args.train_mask_frames = 1\n",
    "args.output_activation = 'none'\n",
    "args.use_first_last = False\n",
    "args.down_up_sample = False\n",
    "args.pca_ckpt = \"/home/ubuntu/DinoFeatPred/pca/pca_448_l[2_5_8_11]_1152.pth\"\n",
    "args.crop_feats = False\n",
    "args.sliding_window_inference = False\n",
    "args.use_bn = True\n",
    "args.use_cls = False\n",
    "args.nfeats = 256\n",
    "args.dpt_out_channels = [128,256,512,512]\n",
    "# Training parameters\n",
    "args.max_epochs = 800\n",
    "args.seed = 123\n",
    "args.single_step_sample_train = True\n",
    "args.precision = '32-true'\n",
    "args.ckpt = None\n",
    "args.num_gpus = 1\n",
    "args.accum_iter = 1\n",
    "args.warmup_p = 0.0\n",
    "args.lr_base = 1e-3\n",
    "args.weight_decay = 0\n",
    "args.scheduler = \"cosine\"\n",
    "args.optimizer = \"adam\"\n",
    "args.gclip = 1.0\n",
    "args.evaluate = True\n",
    "args.eval_midterm = False\n",
    "args.eval_mode = True\n",
    "args.use_val_to_train = False\n",
    "args.use_train_to_val = False\n",
    "args.vis_attn = True\n",
    "\n",
    "obj_colors_dict = {\n",
    "    0: [128, 64, 128],\n",
    "    1: [244, 35, 232],\n",
    "    2: [70, 70, 70],\n",
    "    3: [102, 102, 156],\n",
    "    4: [190, 153, 153],\n",
    "    5: [153, 153, 153],\n",
    "    6: [250, 170, 30],\n",
    "    7: [220, 220, 0],\n",
    "    8: [107, 142, 35],\n",
    "    9: [152, 251, 152],\n",
    "    10: [70, 130, 180],\n",
    "    11: [220, 20, 60],\n",
    "    12: [255, 0, 0],\n",
    "    13: [0, 0, 142],\n",
    "    14: [0, 0, 70],\n",
    "    15: [0, 60, 100],\n",
    "    16: [0, 80, 100],\n",
    "    17: [0, 0, 230],\n",
    "    18: [119, 11, 32]\n",
    "}\n",
    "args.class_colors_arr = np.array(list(obj_colors_dict.values()),dtype=np.uint8)\n",
    "\n",
    "args.device = 'cuda:7' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEG\n",
    "args.num_classes = 19\n",
    "args.eval_modality = \"segm\"\n",
    "args.head_ckpt = \"/path/to/Checkpoints/head_pca1152.ckpt\"\n",
    "\n",
    "# # Depth \n",
    "# args.num_classes = 256\n",
    "# args.eval_modality = \"depth\"\n",
    "# args.head_ckpt = \"/path/to/Checkpoints/head_depth_pca1152.ckpt\"\n",
    "\n",
    "# # Normals\n",
    "# args.num_classes = 3\n",
    "# args.eval_modality = \"surface_normals\"\n",
    "# args.head_ckpt = \"/path/to/Checkpoints/head_normals_pca1152.ckpt\"\n",
    "\n",
    "dataset = CS_VideoData(arguments=args,subset=\"val\",batch_size=args.batch_size)\n",
    "val_dl = dataset.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = \"/path/to/dinof_highres.ckpt\"\n",
    "model = Dino_f.load_from_checkpoint(ckpt_path,args=args,strict=False).to('cuda:7')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = torch.randint(low=0,high=500,size=(1,))\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 2 # ==>> 341, 223, 99  # 24 turn  [13, 26, 266, 276,  333 ]\n",
    "for i, batch in enumerate(val_dl):\n",
    "    # data, gt_img, gt_modal = batch\n",
    "    data, gt_img, gt_modal, gt_future_img, gt_path = batch\n",
    "    print(i, gt_path)\n",
    "    if i == s:\n",
    "        break\n",
    "B, sl, C, H, W = data.shape\n",
    "print(data.shape)\n",
    "print(gt_img.shape)\n",
    "print(gt_modal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=args.sequence_length, figsize=(20, 5))\n",
    "denorm_frames = denormalize_images(data)\n",
    "for i in range(args.sequence_length):\n",
    "    axes[i].imshow(denorm_frames[0,i].permute(1,2,0).cpu().numpy())\n",
    "    axes[i].axis('off')\n",
    "    axes[i].set_title(f\"Frame {i}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.eval_modality == \"segm\":\n",
    "    vis_gt_segm = np.zeros((1024,2048,3),dtype=np.uint8)\n",
    "    gt_mask = gt_modal.squeeze()!=IGNORE_LABEL\n",
    "    vis_gt_segm[gt_mask,:] =  args.class_colors_arr[gt_modal.squeeze()[gt_mask]]\n",
    "    T.ToPILImage(mode='RGB')(vis_gt_segm)\n",
    "    plt.imshow(vis_gt_segm)\n",
    "    plt.show()\n",
    "elif args.eval_modality == \"depth\":\n",
    "    plt.imshow(gt_modal.squeeze(),cmap='turbo')\n",
    "    plt.show()\n",
    "elif args.eval_modality == \"surface_normals\":\n",
    "    plt.imshow(F.normalize(gt_modal.squeeze(),p=2,dim=0).cpu().numpy().transpose(1,2,0))\n",
    "    # plt.imshow(gt_modal.squeeze().cpu().numpy().transpose(1,2,0))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    x = model.preprocess(data.to(args.device))\n",
    "    print(\"PCA_feats shape\",x.shape)\n",
    "    masked_soft_tokens, mask = model.get_mask_tokens(x, mode=\"full_mask\",mask_frames=1)\n",
    "    mask = mask.to(x.device)\n",
    "    if model.args.vis_attn:\n",
    "        _, final_tokens, attn_weights = model.forward(x, masked_soft_tokens, mask)\n",
    "    else:\n",
    "        loss, final_tokens = model.forward(x, masked_soft_tokens, mask)\n",
    "        print(loss)\n",
    "    prediction = model.postprocess(final_tokens)\n",
    "print(prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothl1 = nn.SmoothL1Loss(reduction='none', beta=0.1)\n",
    "loss_tokens = smoothl1(final_tokens[:,-1],x[:,-1]).mean(dim=-1)\n",
    "plt.imshow(loss_tokens.squeeze().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTED IMAGE\n",
    "pred_feats = prediction[:,-1]\n",
    "pred_feats_list = [pred_feats[:,:,:,i*model.feature_dim:(i+1)*model.feature_dim] for i in range(model.d_num_layers)]\n",
    "pred_feats_list = [einops.rearrange(x, 'b h w c -> b (h w) c',h=H//model.patch_size, w=W//model.patch_size) for x in pred_feats_list]\n",
    "pred_modal = model.head(pred_feats_list,model.patch_h,model.patch_w)\n",
    "pred_modal = F.interpolate(pred_modal, size=(1024,2048), mode='bilinear', align_corners=False)\n",
    "# ORACLE IMAGE\n",
    "oracle_feats = model.extract_features(gt_img.to(args.device))\n",
    "oracle_feats_list = [oracle_feats[:,:,i*model.feature_dim:(i+1)*model.feature_dim] for i in range(model.d_num_layers)]\n",
    "oracle_pred = model.head(oracle_feats_list,model.patch_h,model.patch_w)\n",
    "oracle_pred = F.interpolate(oracle_pred, size=(1024,2048), mode='bilinear', align_corners=False)\n",
    "if args.eval_modality == \"segm\":\n",
    "    pred_modal = torch.argmax(pred_modal, dim=1)\n",
    "    pred_modal_rgb = args.class_colors_arr[pred_modal.squeeze().cpu().numpy()]\n",
    "    plt.figure()\n",
    "    plt.imshow(T.ToPILImage(mode='RGB')(pred_modal_rgb))\n",
    "    oracle_pred = torch.argmax(oracle_pred, dim=1)\n",
    "    oracle_pred_rgb = args.class_colors_arr[oracle_pred.squeeze().cpu().numpy()]\n",
    "    plt.figure()\n",
    "    plt.imshow(T.ToPILImage(mode='RGB')(oracle_pred_rgb))\n",
    "elif args.eval_modality == \"depth\":\n",
    "    pred_modal = torch.argmax(pred_modal, dim=1)\n",
    "    pred_modal_rgb = cm.turbo(pred_modal.squeeze().cpu().numpy())[...,:3]\n",
    "    plt.figure()\n",
    "    plt.imshow(pred_modal_rgb)\n",
    "    oracle_pred = torch.argmax(oracle_pred, dim=1)\n",
    "    oracle_pred_rgb = cm.turbo(oracle_pred.squeeze().cpu().numpy())[...,:3]\n",
    "    plt.figure()\n",
    "    plt.imshow(oracle_pred_rgb)\n",
    "elif args.eval_modality == \"surface_normals\":\n",
    "    pred_modal_rgb = F.normalize(pred_modal,p=2,dim=1).cpu().numpy().transpose(0,2,3,1)\n",
    "    plt.figure()\n",
    "    plt.imshow(pred_modal_rgb.squeeze())\n",
    "    oracle_pred_rgb = F.normalize(oracle_pred,p=2,dim=1).cpu().numpy().transpose(0,2,3,1)\n",
    "    plt.figure()\n",
    "    plt.imshow(oracle_pred_rgb.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = osp.basename(gt_path[0])\n",
    "unroll_steps = 1 # [1 or 3]\n",
    "assert unroll_steps in [1,3]\n",
    "with torch.no_grad():\n",
    "    x = model.preprocess(data.to(args.device))\n",
    "    print(x.shape)\n",
    "    for i in range(unroll_steps):\n",
    "        masked_soft_tokens, mask = model.get_mask_tokens(x, mode=\"full_mask\",mask_frames=1)\n",
    "        mask = mask.to(x.device)\n",
    "        if model.args.vis_attn:\n",
    "            _, final_tokens, attn_weights = model.forward(x, masked_soft_tokens, mask)\n",
    "        else:\n",
    "            loss, final_tokens = model.forward(x, masked_soft_tokens, mask)\n",
    "        x[:,-1] = final_tokens[:,-1]\n",
    "        x[:,0:-1] = x[:,1:].clone()\n",
    "    prediction = model.postprocess(x)\n",
    "time = '(t+'+str(unroll_steps*3)+')'\n",
    "modal_name =  name.replace(\"leftImg8bit\",\"pred_\"+args.eval_modality+time)\n",
    "oracle_modal_name = name.replace(\"leftImg8bit\",\"oracle_\"+args.eval_modality+time)\n",
    "# PREDICTED IMAGE\n",
    "pred_feats = prediction[:,-1]\n",
    "pred_feats_list = [pred_feats[:,:,:,i*model.feature_dim:(i+1)*model.feature_dim] for i in range(model.d_num_layers)]\n",
    "pred_feats_list = [einops.rearrange(x, 'b h w c -> b (h w) c',h=H//model.patch_size, w=W//model.patch_size) for x in pred_feats_list]\n",
    "pred_modal = model.head(pred_feats_list,model.patch_h,model.patch_w)\n",
    "pred_modal = F.interpolate(pred_modal, size=(1024,2048), mode='bilinear', align_corners=False)\n",
    "# ORACLE IMAGE\n",
    "if unroll_steps == 1:\n",
    "    oracle_feats = model.extract_features(gt_img.to(args.device))\n",
    "    oracle_feats_list = [oracle_feats[:,:,i*model.feature_dim:(i+1)*model.feature_dim] for i in range(model.d_num_layers)]\n",
    "    oracle_pred = model.head(oracle_feats_list,model.patch_h,model.patch_w)\n",
    "    oracle_pred = F.interpolate(oracle_pred, size=(1024,2048), mode='bilinear', align_corners=False)\n",
    "elif unroll_steps == 3:\n",
    "    # Future Image\n",
    "    future_feats = model.extract_features(gt_future_img.to(args.device))\n",
    "    future_feats_list = [future_feats[:,:,i*model.feature_dim:(i+1)*model.feature_dim] for i in range(model.d_num_layers)]\n",
    "    future_pred = model.head(future_feats_list,model.patch_h,model.patch_w)\n",
    "    future_pred = F.interpolate(future_pred, size=(1024,2048), mode='bilinear', align_corners=False)\n",
    "else:\n",
    "    assert False\n",
    "if args.eval_modality == \"segm\":\n",
    "    pred_modal = torch.argmax(pred_modal, dim=1)\n",
    "    pred_modal_rgb = args.class_colors_arr[pred_modal.squeeze().cpu().numpy()]\n",
    "    plt.figure()\n",
    "    plt.imshow(T.ToPILImage(mode='RGB')(pred_modal_rgb))\n",
    "    T.ToPILImage(mode='RGB')(pred_modal_rgb).save(modal_name)\n",
    "    if unroll_steps == 1:\n",
    "        oracle_pred = torch.argmax(oracle_pred, dim=1)\n",
    "        oracle_pred_rgb = args.class_colors_arr[oracle_pred.squeeze().cpu().numpy()]\n",
    "        plt.figure()\n",
    "        plt.imshow(T.ToPILImage(mode='RGB')(oracle_pred_rgb))\n",
    "        T.ToPILImage(mode='RGB')(oracle_pred_rgb).save(oracle_modal_name)\n",
    "    elif unroll_steps==3:\n",
    "        future_pred = torch.argmax(future_pred, dim=1)\n",
    "        future_pred_rgb = args.class_colors_arr[future_pred.squeeze().cpu().numpy()]\n",
    "        plt.figure()\n",
    "        plt.imshow(T.ToPILImage(mode='RGB')(future_pred_rgb))\n",
    "        T.ToPILImage(mode='RGB')(future_pred_rgb).save(oracle_modal_name)\n",
    "elif args.eval_modality == \"depth\":\n",
    "    pred_modal = torch.argmax(pred_modal, dim=1)\n",
    "    pred_modal_rgb = cm.turbo(pred_modal.squeeze().cpu().numpy())[...,:3]\n",
    "    plt.figure()\n",
    "    plt.imshow(pred_modal_rgb)\n",
    "    T.ToPILImage()(pred_modal_rgb).save(modal_name)\n",
    "    if unroll_steps == 1:\n",
    "        oracle_pred = torch.argmax(oracle_pred, dim=1)\n",
    "        oracle_pred_rgb = cm.turbo(oracle_pred.squeeze().cpu().numpy())[...,:3]\n",
    "        plt.figure()\n",
    "        plt.imshow(oracle_pred_rgb)\n",
    "        T.ToPILImage()(oracle_pred_rgb).save(oracle_modal_name)\n",
    "    elif unroll_steps==3:\n",
    "        future_pred = torch.argmax(future_pred, dim=1)\n",
    "        future_pred_rgb = cm.turbo(future_pred.squeeze().cpu().numpy())[...,:3]\n",
    "        plt.figure()\n",
    "        plt.imshow(future_pred_rgb)\n",
    "        T.ToPILImage()(future_pred_rgb).save(oracle_modal_name)\n",
    "elif args.eval_modality == \"surface_normals\":\n",
    "    pred_modal_rgb = F.normalize(pred_modal,p=2,dim=1).cpu().numpy().transpose(0,2,3,1)\n",
    "    plt.figure()\n",
    "    plt.imshow(pred_modal_rgb.squeeze())\n",
    "    T.ToPILImage()(pred_modal_rgb.squeeze()).save(modal_name)\n",
    "    if unroll_steps == 1:\n",
    "        oracle_pred_rgb = F.normalize(oracle_pred,p=2,dim=1).cpu().numpy().transpose(0,2,3,1)\n",
    "        plt.figure()\n",
    "        plt.imshow(oracle_pred_rgb.squeeze())\n",
    "        T.ToPILImage()(oracle_pred_rgb.squeeze()).save(oracle_modal_name)\n",
    "    elif unroll_steps==3:\n",
    "        future_pred_rgb = F.normalize(future_pred,p=2,dim=1).cpu().numpy().transpose(0,2,3,1)\n",
    "        plt.figure()\n",
    "        plt.imshow(future_pred_rgb.squeeze())\n",
    "        T.ToPILImage()(future_pred_rgb.squeeze()).save(oracle_modal_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = osp.basename(gt_path[0])\n",
    "f_name = name.split('_leftImg8bit')[0]\n",
    "unroll_steps = 16 # [1 or 3]\n",
    "context_length = data.shape[1] - 1\n",
    "denorm_data = denormalize_images(data)\n",
    "for i in range(context_length):\n",
    "    # Save predictions\n",
    "    time = '(t-'+str(9-i*3)+')'\n",
    "    context_frame = denorm_data[0, i] # Assuming batch size 1\n",
    "    ctx_name = name.replace(\"leftImg8bit\",\"context\"+time)\n",
    "    ctx_path =  osp.join(\"/home/ubuntu/Dino_Predictions_Unroll\",f_name, ctx_name)\n",
    "    os.makedirs(osp.dirname(ctx_path),exist_ok=True)\n",
    "    T.ToPILImage()(context_frame).save(ctx_path)\n",
    "with torch.no_grad():\n",
    "    x = model.preprocess(data.to(args.device))\n",
    "    print(x.shape)\n",
    "    for i in range(unroll_steps):\n",
    "        masked_soft_tokens, mask = model.get_mask_tokens(x, mode=\"full_mask\",mask_frames=1)\n",
    "        mask = mask.to(x.device)\n",
    "        if model.args.vis_attn:\n",
    "            _, final_tokens, attn_weights = model.forward(x, masked_soft_tokens, mask)\n",
    "        else:\n",
    "            loss, final_tokens = model.forward(x, masked_soft_tokens, mask)\n",
    "        x[:,-1] = final_tokens[:,-1]\n",
    "        x[:,0:-1] = x[:,1:].clone()\n",
    "        prediction = model.postprocess(x)\n",
    "        time = '(t+'+str(3+i*3)+')'\n",
    "        modal_name =  name.replace(\"leftImg8bit\",\"pred_\"+args.eval_modality+time)\n",
    "        modal_path =  osp.join(\"/home/ubuntu/Dino_Predictions_Unroll\",f_name, modal_name)\n",
    "        os.makedirs(osp.dirname(modal_path),exist_ok=True)\n",
    "        # PREDICTED IMAGE\n",
    "        pred_feats = prediction[:,-1]\n",
    "        pred_feats_list = [pred_feats[:,:,:,i*model.feature_dim:(i+1)*model.feature_dim] for i in range(model.d_num_layers)]\n",
    "        pred_feats_list = [einops.rearrange(x, 'b h w c -> b (h w) c',h=H//model.patch_size, w=W//model.patch_size) for x in pred_feats_list]\n",
    "        pred_modal = model.head(pred_feats_list,model.patch_h,model.patch_w)\n",
    "        pred_modal = F.interpolate(pred_modal, size=(1024,2048), mode='bilinear', align_corners=False)\n",
    "        pred_modal = torch.argmax(pred_modal, dim=1)\n",
    "        pred_modal_rgb = args.class_colors_arr[pred_modal.squeeze().cpu().numpy()]\n",
    "        T.ToPILImage(mode='RGB')(pred_modal_rgb).save(modal_path)\n",
    "        # pred_modal = torch.argmax(pred_modal, dim=1)\n",
    "        # pred_modal_rgb = cm.turbo(pred_modal.squeeze().cpu().numpy())[...,:3]\n",
    "        # T.ToPILImage(mode='RGB')(pred_modal_rgb).save(modal_path)\n",
    "        # pred_modal_rgb = F.normalize(pred_modal,p=2,dim=1).cpu().numpy().transpose(0,2,3,1)\n",
    "        # T.ToPILImage()(pred_modal_rgb.squeeze()).save(modal_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fpred",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
